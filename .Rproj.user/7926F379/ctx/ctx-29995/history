plot_ly(data=data,x=~y  )%>%
layout(title="Histograma de compras mensuales por cliente",
yaxis=list(title="Frecuencia"),xaxis=list(title="Histograma de compras mensuales por cliente"))
lim_sup<- quantile(data$y,0.75) +(IQR(data$y)*1.5 )
lim_inf<- quantile(data$y,0.25) - (IQR(data$y)*1.5 )
data=subset(data, data$y<=lim_sup & data$y>lim_inf)
cat<-c(3,9,18,17,10,20,21)
data_cat<-data[ ,cat]
data_num<-data[,-cat]
####### analizar categoricas concluir cuáles tiene influencia
boxplot(data$y~data$dia_sem_mas_frecuente)
boxplot(data$y~data$genero)
plot_ly(data=data, y=data$y, x=data$genero, type="box")
plot_ly(data=data, y=data$y, x=data$reportado_data_credito, type="box")
plot_ly(data=data, y=data$y, x=data$cat_principal, type="box")
plot_ly(data=data, y=data$y, x=data$medio_apgo, type="box")
###Analizar correlaciones
mc<-cor(data_num)
corrplot(mc,type="upper")
### 5. atipicos de variables explicativas ####
data=data[,-c(20,21)]
mod<-lm(data=data,y~.)
predichos<-predict(mod)
mape(data$y,predichos)
datos_influyentes<- influence.measures(mod)
datos_influyentes<- influence.measures(mod)
datos_influyentes
influyentes<-summary(datos_influyentes)
filas_filtrar<-as.numeric(row.names(influyentes))
filas_filtrar
data=data[-filas_filtrar,]
length(filas_filtrar)
mod<-lm(data=data,y~.)
predichos<-predict(mod)
mape(data$y,predichos)
mod<-lm(data=data,y~.)
predichos<-predict(mod)
mape(data$y,predichos)
library(corrplot) ###Gráfico de correlaciones
library(plotly) ###para gráficos bonitos
library(Metrics) ### para calcular mape
library(regclass) ### para VIF
library(car)  ### validacion supuestos
library(MASS) ### para stepAIC
library(tseries) ### validacion supuestos
library(nortest) ### validacion supuestos
library(goftest) ### validacion supuestos
library(lmtest) ### validacion supuestos
library(randomForest) #### para imputación de datos faltantes
library(dplyr) #### para case_when para reagrupar variables
library(skimr)
data_original<-read.csv("base_supermercado.csv",dec=",",sep=";")
# Exploración inicial, verificar atípicos, números de categorías
skim(data_original)
library(corrplot) ###Gráfico de correlaciones
library(plotly) ###para gráficos bonitos
library(Metrics) ### para calcular mape
library(regclass) ### para VIF
library(car)  ### validacion supuestos
library(MASS) ### para stepAIC
library(tseries) ### validacion supuestos
library(nortest) ### validacion supuestos
library(goftest) ### validacion supuestos
library(lmtest) ### validacion supuestos
library(randomForest) #### para imputación de datos faltantes
library(dplyr) #### para case_when para reagrupar variables
library(skimr)
data_original<-read.csv("base_supermercado.csv",dec=",",sep=";")
data_original$cat_principal= as.factor(data_original$cat_principal)
data_original$genero=as.factor(data_original$genero)
data_original$reportado_data_credito= as.factor(data_original$reportado_data_credito)
data_original$medio_apgo= as.factor(data_original$medio_apgo)
summary(data_original)
table(is.na(data_original))
data_original<-na.roughfix(data_original)
data_original$reportado_data_credito= as.factor(data_original$reportado_data_credito)
data_original$medio_apgo= as.factor(data_original$medio_apgo)
data_original$dia_sem_mas_frecuente= as.factor(data_original$dia_sem_mas_frecuente)
data_original$producto_frecuente = as.factor(data_original$producto_frecuente)
data_original$fecha_ultima_compra<-as.factor(data_original$fecha_ultima_compra)
data_original<-na.roughfix(data_original)
table(is.na(data_original))
obs_gen<-table(data_original$genero)
obs_gen
obs_rep<-table(data_original$reportado_data_credito)
obs_rep[order(obs_rep,decreasing=T)]
data$reportado_data_credito<-case_when(
data$reportado_data_credito=="SI"~"Reportado",
TRUE~"No reportado")
data$reportado_data_credito<-case_when(
data$reportado_data_credito=="SI"~"Reportado"
TRUE~"No reportado")
data$reportado_data_credito<-case_when(
data$reportado_data_credito=="SI"~"Reportado"
TRUE~"No reportado")
data$reportado_data_credito<-case_when(
data$reportado_data_credito=="SI"~"Reportado"
TRUE~"No reportado")
data$reportado_data_credito<-case_when(
data$reportado_data_credito=="SI"~"Reportado",
TRUE~"No reportado")
data_original$medio_apgo<-case_when(
data_original$medio_apgo=="TD"~"TD",
data_original$medio_apgo=="TDC"~"TDC",
TRUE~"Efectivo")
data_original$reportado_data_credito<-case_when(
data_original$reportado_data_credito=="SI"~"Reportado",
TRUE~"No reportado")
table(data_original$reportado_data_credito)
data_original<-read.csv("base_supermercado.csv",dec=",",sep=";")
### slide 6  Describir datos y analizar los tipos de variables ####
#####Revisar que las variables queden en el formato correcto
str(data_original)
# Exploración inicial, verificar atípicos, números de categorías
skim(data_original)
###slide 7 Convertir los variables que se consideren necesarias a factores ####
data_original$cat_principal= as.factor(data_original$cat_principal)
data_original$genero=as.factor(data_original$genero)
data_original$reportado_data_credito= as.factor(data_original$reportado_data_credito)
data_original$medio_apgo= as.factor(data_original$medio_apgo)
data_original$dia_sem_mas_frecuente= as.factor(data_original$dia_sem_mas_frecuente)
data_original$producto_frecuente = as.factor(data_original$producto_frecuente)
data_original$fecha_ultima_compra<-as.factor(data_original$fecha_ultima_compra)
str(data_original)
####Una variable con muchas categorías es difícil de analizar, puede hacer el modelo lento y
#### generar sobre ajustes, aunque no hay un número exacto para saber qué son muchas, podría utilizarse más de 20 o 30 como referencia
#### según el estudio este valor se puede aumentar o disminuir
###Además se debe revisar que ninguna categoría tenga pocas observaciones
summary(data_original)
#### slide 8. Tratar datos faltantes ####
####Faltantes-imputación
table(is.na(data_original))
data_original<-na.roughfix(data_original)
table(is.na(data_original))
#############Slide 9 revisar categorías en variables categóricas
obs_gen<-table(data_original$genero)
obs_gen[order(obs_gen,decreasing=T)]
obs_rep<-table(data_original$reportado_data_credito)
obs_rep[order(obs_rep,decreasing=T)]
### Dado que tiene una categoría con una sola observación se reagrupa:
###Si la base de entrenamiento y evaluación está separada, la reagrupación se hace en las dos bases
data_original$reportado_data_credito<-case_when(
data_original$reportado_data_credito=="SI"~"Reportado",
TRUE~"No reportado")
table(data_original$reportado_data_credito)
table(data_original$reportado_data_credito)
data$reportado_data_credito= as.factor(data$reportado_data_credito) ### para que en el summary quede bien
data_original$reportado_data_credito= as.factor(data$reportado_data_credito) ### para que en el summary quede bien
data_original$reportado_data_credito= as.factor(data_original$reportado_data_credito) ### para que en el summary quede bien
obs_cat<-table(data_original$cat_principal)
obs_cat[order(obs_cat,decreasing=T)]
obs_med<-table(data_original$medio_apgo)
obs_med[order(obs_med,decreasing=T)]
data_original$medio_apgo<-case_when(
data_original$medio_apgo=="TD"~"TD",
data_original$medio_apgo=="TDC"~"TDC",
TRUE~"Efectivo")
table(data_original$medio_apgo)
data_original$medio_apgo= as.factor(data_original$medio_apgo) ### para que en el summary quede bien
obs_diasem<-table(data_original$dia_sem_mas_frecuente)
obs_diasem[order(obs_diasem,decreasing=T)]
obs_prod<-table(data_original$producto_frecuente)
obs_prod[order(obs_prod,decreasing=T)]
data_original$producto_frecuente =
case_when (
data_original$producto_frecuente == "Pollo" ~"Pollo",
data_original$producto_frecuente == "Carne" ~"Carne",
data_original$producto_frecuente == "Cerveza tibetana Barley" ~"Cerveza",
TRUE ~ "Otros"
)
data_original$producto_frecuente=as.factor(data_original$producto_frecuente)
View(data_original)
data_original$fecha_ultima_compra2<-as.Date(data_original$fecha_ultima_compra,"%d/%m/%Y") ##se convierte la columna a formato fecha
data_original$fu_mes=months.Date(data_original$fecha_ultima_compra2, abbreviate = T) ### para extraer el mes
View(data_original)
data_original$fu_ds=weekdays(data_original$fecha_ultima_compra2, abbreviate = T) ###para extraer el día de la semana
data_original$fu_tri=quarters(data_original$fecha_ultima_compra2, abbreviate = T) ### Para extraer el trimestre
data_original$fu_dm=format(data_original$fecha_ultima_compra2, format="%d") ###Para extraer el día del mes
data_original$fu_a=format(data_original$fecha_ultima_compra2, format="%Y") ## para extraer el año
data_original$fu_mes=as.factor(data_original$fu_mes)
data_original$fu_ds=as.factor(data_original$fu_ds)
data_original$fu_tri=as.factor(data_original$fu_tri)
data_original$fu_dm=as.factor(data_original$fu_dm)
data_original$fu_a=as.factor(data_original$fu_a)
data<-data_original[,-c(1,"fecha_ultima_compra",22) ] #Eliminar variables repetidas y id que no se va a usar.
data<-data_original[,-"fecha_ultima_compra" ] #Eliminar variables repetidas y id que no se va a usar.
filas_eliminar=c("fecha_ultima_compra","fecha_ultima_compra")
filas_eliminar=c("fecha_ultima_compra","fecha_ultima_compra2")
data<-data_original[,-filas_eliminar ] #Eliminar variables repetidas y id que no se va a usar.
filas_eliminar<-c("fecha_ultima_compra","fecha_ultima_compra2")
data<-data_original[,-filas_eliminar ] #Eliminar variables repetidas y id que no se va a usar.
data<-data_original[,filas_eliminar ] #Eliminar variables repetidas y id que no se va a usar.
data<-data_original[,-c(1,20,22) ] #Eliminar variables repetidas y id que no se va a usar.
hist(data$y/1000000, main= "Histograma de compras mensuales por cliente",
ylab="Frecuencia", xlab="Promedio compras mensuales por cliente (millones de pesos)") ##
hist(data$y/1000000, main= "Histograma de compras mensuales por cliente",
ylab="Frecuencia", xlab="Promedio compras mensuales por cliente (millones de pesos)") ##
hist(data$y/1000000, main= "Histograma de compras mensuales por cliente",
ylab="Frecuencia", xlab="Promedio compras mensuales por cliente (millones de pesos)") ##
boxplot(data$y, horizontal = T)
qqPlot(data$y)
jarque.bera.test(data$y)
plot_ly(data=data,x=~y  )%>%
layout(title="Histograma de compras mensuales por cliente",
yaxis=list(title="Frecuencia"),xaxis=list(title="Histograma de compras mensuales por cliente"))
cat<-c(3,9,18,17,10,20,21)
data_cat<-data[ ,cat]
data_num<-data[,-cat]
####### analizar categoricas concluir cuáles tiene influencia
boxplot(data$y~data$dia_sem_mas_frecuente)
boxplot(data$y~data$genero)
plot_ly(data=data, y=data$y, x=data$genero, type="box")
plot_ly(data=data, y=data$y, x=data$reportado_data_credito, type="box")
plot_ly(data=data, y=data$y, x=data$cat_principal, type="box")
plot_ly(data=data, y=data$y, x=data$medio_apgo, type="box")
###Analizar correlaciones
mc<-cor(data_num)
corrplot(mc,type="upper")
cat<-c(3,9,18,17,10,20,21)
data_cat<-data[ ,cat]
data_num<-data[,-cat]
cat<-c(2,8,17,16,9,19,20)
data_cat<-data[ ,cat]
data_cat<-data[ ,cat]
data_num<-data[,-cat]
cat<-c(2,8,17,16,9,19,20,21,22,23,24)
data_cat<-data[ ,cat]
data_num<-data[,-cat]
####### analizar categoricas concluir cuáles tiene influencia
boxplot(data$y~data$dia_sem_mas_frecuente)
boxplot(data$y~data$genero)
plot_ly(data=data, y=data$y, x=data$genero, type="box")
plot_ly(data=data, y=data$y, x=data$reportado_data_credito, type="box")
plot_ly(data=data, y=data$y, x=data$cat_principal, type="box")
plot_ly(data=data, y=data$y, x=data$medio_apgo, type="box")
###Analizar correlaciones
mc<-cor(data_num)
corrplot(mc,type="upper")
###Analizar correlaciones
mc<-cor(data_num)
corrplot(mc,type="upper")
dim(data)
lim_sup<- quantile(data$y,0.75) +(IQR(data$y)*1.5 )
lim_inf<- quantile(data$y,0.25) - (IQR(data$y)*1.5 )
data=subset(data, data$y<=lim_sup & data$y>lim_inf)
dim(data)
mod<-lm(y~.,data=data)
y1<-data$y
datos_influyentes<- influence.measures(mod)
influyentes<-summary(datos_influyentes)
filas_filtrar<-as.numeric(row.names(influyentes))
data2<-data[-filas_filtrar, ] ### datos eliminando atípicos
mod2<-lm(y~.,data=data2)
predichos2<-predict(mod2)
y2=data2$y
dim(data)
dim(data2)
mape(y1,predichos) # mape entrenamiento antes de eliminar inusuales
mape(y2,predichos2) #mape entrenamiento después de eliminar inusuales
mape(y1,predichos) # mape entrenamiento antes de eliminar inusuales
predichos<-predict(mod)
y1<-data$y
datos_influyentes<- influence.measures(mod)
influyentes<-summary(datos_influyentes)
filas_filtrar<-as.numeric(row.names(influyentes))
data2<-data[-filas_filtrar, ] ### datos eliminando atípicos
mod2<-lm(y~.,data=data2)
predichos2<-predict(mod2)
y2=data2$y
dim(data)
dim(data2)
mape(y1,predichos) # mape entrenamiento antes de eliminar inusuales
mape(y2,predichos2) #mape entrenamiento después de eliminar inusuales
set.seed(987)
filas_train<-sample(1:length(data2$y),39000)
data_sinid<-data2[filas_train,]###asignar base de entrenamiento  eliminando variable id
data_test_sinid<-data2[-filas_train,] #### guardar variable de evaluaciÃ³n
data_sinid<-data2[filas_train,]###asignar base de entrenamiento  eliminando variable id
modelo<-lm(data=data_sinid,y~.)
summary(modelo)
mape(data_test_sinid$y, predict(modelo,data_test_sinid))
summary(modelo)
mape(data_test_sinid$y, predict(modelo,data_test_sinid))
data_dep=data_sinid[,-c(12,15,22)]
modelo2<-lm(data=data_dep,y~.)
summary(modelo2)
mape(data_test_sinid$y, predict(modelo2,data_test_sinid))
modelo<-lm(data=data_sinid,y~.)
summary(modelo)
mape(data_test_sinid$y, predict(modelo,data_test_sinid))
mape(data_test_sinid$y, predict(modelo2,data_test_sinid))
vifs<-data.frame(VIF(modelo2))
vifs[order(vifs$GVIF, decreasing=T),]
vifs
vifs[order(vifs$GVIF, decreasing=T),]
predict_test<-predict(modelo2,data_test_sinid)
mape(data_test_sinid$y,predict_test)
rmse(data_test_sinid$y,predict_test)
data_dep2=data_dep[,-c(1,5)]
modelo3<-lm(data=data_dep2,y~.)
summary(modelo3)
vifs<-data.frame(VIF(modelo3))
vifs[order(vifs$GVIF, decreasing=T),]
predict_test<-predict(modelo2,data_test_sinid)
mape(data_test_sinid$y,predict_test)
rmse(data_test_sinid$y,predict_test)
predict_test<-predict(modelo3,data_test_sinid)
mape(data_test_sinid$y,predict_test)
rmse(data_test_sinid$y,predict_test)
modelo_reducido<-stepAIC(modelo2)
#Calculo de indicador en muestra de entrenamiento
mape(data_dep$y,predict(modelo_reducido))
rmse(data_dep$y,predict(modelo_reducido))
AIC(modelo_reducido)
BIC(modelo_reducido)
predict_test<-predict(modelo_reducido,data_test_sinid)
mape(data_test_sinid$y,predict_test)
rmse(data_test_sinid$y,predict_test)
predict_test<-predict(modelo2,data_test_sinid)
mape(data_test_sinid$y,predict_test)
rmse(data_test_sinid$y,predict_test)
# 1 Supuesto de normalidad -Graficos
#Histograma
par(mfrow=c(1,3))
hist(residuales)
#qqplot fancy
qqPlot(residuales) # Si se salen de las bandas generalmente no cumple normalidad
# 1 Supuesto de normalidad -Graficos
#Histograma
par(mfrow=c(1,3))
hist(residuales)
#Guardar residuales en viarble para analizarlo
residuales<-modelo_reducido$residuals
# 1 Supuesto de normalidad -Graficos
#Histograma
par(mfrow=c(1,3))
hist(residuales)
#qqplot fancy
qqPlot(residuales) # Si se salen de las bandas generalmente no cumple normalidad
# boxplot
boxplot(residuales, horizontal = T)
jarque.bera.test(residuales) #jarque bera test
ks.test(x=residuales,pnorm,0,sd(residuales)) #kolmogorov smirnov
ad.test(residuales, pnorm, 0, sd(residuales)) #Anderson darling
cvm.test(residuales,pnorm, 0, sd(residuales))  ###Cramer von mises
plot(predict(modelo_reducido),residuales)
abline(h=0)
plot(predict(modelo_reducido),residuales)
abline(h=0)
# 2 supuesto Varianza constante - prueba estadísticas
bptest(modelo_reducido) ##breusch pagan test
par(mfrow=c(1,3))
#Orden vs residuales
plot(residuales)
#acf y pacf
acf(residuales)
pacf(residuales)
#Prueba de durbin watson
dwtest(modelo_reducido)
bgtest(modelo_reducido)
datos <- read_csv("datos.csv")
#Ruta de lectura del archivo:
library(readr)
datos <- read_csv("datos.csv")
View(datos)
#Llamamos librerias:
library(Mcomp)
library(smooth)
library(tseries)
library(Metrics)
library(forecast)
library(car)
#Llamamos la base de datos:
print(datos)
#Transformamos a serie de tiempo:
serie_ts <-ts(datos, start=c(1960,1), frequency=12)
serie_ts
#Verificamos que sea serie de tiempo:
class(serie_ts)
plot(serie_ts, main= "Demanda de gasolina en Ontario, Canad? 1960-1975") #
#Descomposici?n modelo aditivo en R:
descomp<-decompose(serie_ts, type="additive")
plot(descomp)
#Descomposici?n modelo multiplicativo:
descomp2<-decompose(serie_ts, type="multiplicative")
plot(descomp2)
#Verificar la correlaci?n:
acf(serie_ts, main= "ACF")
pacf(serie_ts)
#________________________REGRESI?N LINEAL________________________
serie_ts_train=window(serie_ts,start=c(1960,1),end=c(1972,12)) #Muestra para entrenamiento 80% de los datos
serie_ts_test=window(serie_ts,start=c(1973,1),end=c(1975,12))  #Muestra para evaluaci?n
#Se crean los vectores de los tiempos y los data_frame de entrenamiento y prueba para no tener problema con el formato de la tabla:
t_train=c(1:156)
t_test=c(157:192)
#Verificar que la separaci?n de la serie y la creaci?n de los vectores de tiempo queden bien:
length(serie_ts_train)
length(t_train)
print(serie_ts_train)
length(serie_ts_test)
length(t_test)
print(serie_ts_test)
#Se crean formatos data.frame para entrenamiento y predicci?n:
data_train=data.frame(serie_ts_train,"t"=t_train, "t2"=t_train^2)
data_test=data.frame("t"=t_test, "t2"=t_test^2)
#Se entrena modelo en datos de entrenamiento,entrenar dos para comparar:
modelo1=lm(serie_ts_train~t,data=data_train)     #Entrenar/ajustar modelo
modelo2=lm(serie_ts_train~t2,data=data_train)
summary(modelo1)
summary(modelo2)
#Se realiza predicci?n/pron?stico en datos de entrenamiento usando el modelo creado en el paso anterior:
pred_train=predict(modelo1)
pred_train2=predict(modelo2)
#Se realiza predicci?n en datos nuevos (evaluaci?n)
pred_test=predict(modelo1,newdata=data_test)
pred_test2=predict(modelo2,newdata=data_test)
#Se calcula el mape de los datos reales de entrenamiento vs los predichos de entrenamiento:
forecast::accuracy(modelo1)
forecast::accuracy(modelo2)
print("mape Train")
Metrics::mape(serie_ts_train,pred_train)
Metrics::mape(serie_ts_train,pred_train2)
print("mape Test")
Metrics::mape(serie_ts_test,pred_test)
Metrics::mape(serie_ts_test,pred_test2)
#Comparaci?n de modelos con AIC y BIC
print("AIC de los modelos")
AIC(modelo1)
AIC(modelo2)
print("BIC de los modelos")
BIC(modelo1)
BIC(modelo2)
#Se ajusta el modelo a medias moviles:
m_movi=sma(serie_ts,h=6,silent = F,interval=T, level=0.80)
class(m_movi)
summary(m_movi)
#Mape de entrenamiento Medias M?viles:
ajustado=m_movi$fitted
real=serie_ts
mape(real,ajustado)
#Mape de evaluaci?n Medias M?viles:
sma_f<-function(x,h){
forecast(sma(x,h))
}
e=tsCV(serie_ts,sma_f,h=6)
e
#Pruebas con h=1, 2,3 y 6:
erroresh1<-e[96:192,1]
erroresh2<-e[96:192,2]
erroresh3<-e[96:192,3]
erroresh6<-e[96:192,6]
mapeh1=mean(abs(erroresh1/serie_ts[96:192]),na.rm=T)
mapeh2=mean(abs(erroresh2/serie_ts[96:192]),na.rm=T)
mapeh3=mean(abs(erroresh3/serie_ts[96:192]),na.rm=T)
mapeh6=mean(abs(erroresh6/serie_ts[96:192]),na.rm=T)
mapeh1
mapeh2
mapeh3
mapeh6
#Suavizamiento exponencial:
suavi_exp <-ses(serie_ts,h=6)
summary(suavi_exp)
plot(suavi_exp)
#Mape de Entrenamiento
ajustado=suavi_exp$fitted
real=suavi_exp$x
mape(real,ajustado)
#Mape de Evaluaci?n:
ses_f<-function(x,h){
forecast(ses(x,h))
}
e=tsCV(serie_ts,ses_f,h=6)
e
#Pruebas con h=1, 2, 3 y 6
erroresh1<-e[96:192,1]
erroresh2<-e[96:192,2]
erroresh3<-e[96:192,3]
erroresh6<-e[96:192,6]
mapeh1=mean(abs(erroresh1/serie_ts[96:192]),na.rm=T)
mapeh2=mean(abs(erroresh2/serie_ts[96:192]),na.rm=T)
mapeh3=mean(abs(erroresh3/serie_ts[96:192]),na.rm=T)
mapeh6=mean(abs(erroresh6/serie_ts[96:192]),na.rm=T)
mapeh1
mapeh2
mapeh3
mapeh6
hist(e[,6])
#Suavizamiento exponencial Holt:
suavi_exp_holt <- holt(serie_ts)
plot(suavi_exp_holt)
summary(suavi_exp_holt)
#Mape de entrenamiento un periodo:
ajustado=suavi_exp_holt$fitted
real=suavi_exp_holt$x
##Subir base de datos
Datos_USD <- read_delim("Datos históricos USD_COP .csv", delim = ";", escape_double = FALSE, trim_ws = TRUE)
##Subir base de datos
Datos_USD <- read_delim("Datos históricos USD_COP .csv", delim = ";", escape_double = FALSE, trim_ws = TRUE)
##Subir base de datos
Datos_USD <- read_delim("Datos históricos USD_COP .csv", delim = ";", escape_double = FALSE, trim_ws = TRUE)
install.packages("forecast") ###Modelos de TS, Limpieza de datos y visualización
install.packages("tseries")  ### complemento, formato, modelamiento y gráfico, pruebas de hipotesis
install.packages("lubridate") ###Preprocesamiento de datos, formato fechas y tiempos
install.packages("Metrics")
install.packages("forecast")
install.packages("car")
install.packages("tseries")
install.packages("nortest")
install.packages("goftest")
install.packages("lmtest")
install.packages("smooth") # para medias móviles
install.packages("fpp") ## para conjuntos de datos
